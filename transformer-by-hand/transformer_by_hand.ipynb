{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbvlZysVCsIF"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_bt0--X-MNT"
      },
      "outputs": [],
      "source": [
        "N_CTX = 5\n",
        "N_VOCAB = 2\n",
        "N_EMBED = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EZBvw0u_XKi"
      },
      "outputs": [],
      "source": [
        "Lg = 1024\n",
        "\n",
        "MODEL = {\n",
        "    \"wte\": np.array(\n",
        "        [\n",
        "            [0, 0, 0, 0, 0, 1, 0, 0],  # token `a` (id 0)\n",
        "            [0, 0, 0, 0, 0, 0, 1, 0],  # token `b` (id 1)\n",
        "        ]\n",
        "    ),\n",
        "    \"wpe\": np.array(\n",
        "        [\n",
        "            [1, 0, 0, 0, 0, 0, 0, 0],  # position 0\n",
        "            [0, 1, 0, 0, 0, 0, 0, 0],  # position 1\n",
        "            [0, 0, 1, 0, 0, 0, 0, 0],  # position 2\n",
        "            [0, 0, 0, 1, 0, 0, 0, 0],  # position 3\n",
        "            [0, 0, 0, 0, 1, 0, 0, 0],  # position 4\n",
        "        ]\n",
        "    ),\n",
        "    \"blocks\": [\n",
        "        {\n",
        "            \"attn\": {\n",
        "                \"c_attn\": {  # generates qkv matrix\n",
        "                    \"b\": np.zeros(N_EMBED * 3),\n",
        "                    \"w\": np.array(\n",
        "                        # this is where the magic happens\n",
        "                        # fmt: off\n",
        "                        [\n",
        "                            [\n",
        "                                Lg, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                            [\n",
        "                                Lg, Lg, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                            [\n",
        "                                0.0, Lg, Lg, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                            [\n",
        "                                0.0, 0.0, Lg, Lg, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                            [\n",
        "                                0.0, 0.0, 0.0, Lg, Lg, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                            [\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ],  # v\n",
        "                            [\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1, ],  # v\n",
        "                            [\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # q\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # k\n",
        "                                0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ],  # v\n",
        "                        ]\n",
        "                        # fmt: on\n",
        "                    ),\n",
        "                },\n",
        "                \"c_proj\": {  # weights to project attn result back to embedding space\n",
        "                    \"b\": [0, 0, 0, 0, 0, Lg, 0, 0],\n",
        "                    \"w\": np.array(\n",
        "                        [\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                            [0, 0, 0, 0, 0, -Lg, Lg, 0],\n",
        "                        ]\n",
        "                    ),\n",
        "                },\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjixeqT_Kz9"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def linear(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "\n",
        "def causal_self_attention(x, c_attn, c_proj):\n",
        "    print(x)\n",
        "    x = linear(x, **c_attn)\n",
        "    print(x)\n",
        "\n",
        "    q, k, v = np.split(x, 3, axis=-1)\n",
        "\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
        "\n",
        "    x = attention(q, k, v, causal_mask)\n",
        "\n",
        "    x = linear(x, **c_proj)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, attn):\n",
        "    x = x + causal_self_attention(x, **attn)\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt(inputs, wte, wpe, blocks):\n",
        "    # loop through each input, get embedding by index (0, 1)\n",
        "    token_embeddings = wte[inputs]\n",
        "    # loop through input length, get embedding by (0, len(input))\n",
        "    position_embeddings = wpe[range(len(inputs))]\n",
        "    x = token_embeddings + position_embeddings  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block)\n",
        "\n",
        "    return x @ wte.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22G7rP-gEwsI"
      },
      "outputs": [],
      "source": [
        "CHARS = [\"a\", \"b\"]\n",
        "\n",
        "\n",
        "def tokenise(s):\n",
        "    return [CHARS.index(c) for c in s]\n",
        "\n",
        "\n",
        "def untok(token):\n",
        "    return CHARS[token]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px1frrt9FntQ",
        "outputId": "be276706-cae8-452f-fee6-6589d29672ca"
      },
      "outputs": [],
      "source": [
        "def predict(s):\n",
        "    tokens = tokenise(s)\n",
        "    logits = gpt(np.array(tokens), **MODEL)\n",
        "    print(logits)\n",
        "    probs = softmax(logits)\n",
        "\n",
        "    for i, tok in enumerate(tokens):\n",
        "        pred = np.argmax(probs[i])\n",
        "        print(\n",
        "            f\"{untok(tok)} ({tok}): next={untok(pred)} ({pred}) probs={probs[i]} logits={logits[i]}\"\n",
        "        )\n",
        "\n",
        "    return np.argmax(probs[-1])\n",
        "\n",
        "\n",
        "print(untok(predict(\"aabaa\")))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
